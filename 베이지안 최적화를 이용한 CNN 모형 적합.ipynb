{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAMKC5FEDfP0"
   },
   "source": [
    "## 파이썬을 이용한 딥러닝 - 기말 개인 프로젝트 과제\n",
    "### 이름 : 이은희       학번 : 2020120109\n",
    "\n",
    "---\n",
    " 현대 딥러닝 기술들은 대체로 굉장히 복잡하며 (아주 많은 Hyperparameter의 수) 높은 예측력을 갖도록 설계되어있다.\n",
    "\n",
    "딥러닝 모형은 그 자체로 많은 장점을 가지고 있지만, 크게 2가지 단점을 가지고 있다. \n",
    "\n",
    "> 1. Black Box :\n",
    ">               딥러닝 모형의 의사결정 과정은 설계자 또한 알 수가 없다.\n",
    "> 2. Expensive :\n",
    ">               딥러닝 모형은 많은 층 (Layer)와 Hyperparameter들로 구성되어있기 때문에 \n",
    ">               아주 많은 연산량을 요구한다.\n",
    "\n",
    "딥러닝, 머신러닝 기술의 발전으로 대개의 모형들이 높은 정확도를 보이게 된 현재 시점에서, 많은 딥러닝 모형들은 black box를 투명화 하기 위한 방향으로 더 발전 해 나아가고 있다. \n",
    "\n",
    "본 과제에서는 Black box 모형을 투명화 할 수 있는 하나의 방법으로 'Bayesian Optimizer'를 소개하고 MNIST 데이터에 적용해 보려고 한다.\n",
    "\n",
    "'Bayesian Optimizer'는 모형의 Hyper parameter를 추정하는 과정에 적용되는 방법이다.\n",
    "\n",
    "일반적으로 모형의 Hyper parameter 추정에는 Grid search 나 randomized serach가 사용되지만,\n",
    "Grid search는 Hyper parameter가 아주 많은 딥러닝 모형에 사용하기엔 적합하지 않고,\n",
    "Random search는 확률적 탐색과정으로 Grid search보다 더 빠르고 일반적으로 좋은 결과를 반환하지만, 이전 step 까지의 parameter set의 결과를 전혀 반영하지 않고 다음 후보 parameter set을 고르게 된다.\n",
    "\n",
    "**'Bayesian Optimzier'** 는 random search 와 마찬가지로 parameter 후보군을 확률적으로 고르게 되지만, **이전 parameter set의 결과를 반영하여 다음 후보군을 추천해 준다는 점이 random search와의 차이점이다.**\n",
    "\n",
    "더 자세한 작동 과정은 아래에서 설명하도록 하겠다.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "가장 쉬운 예제 데이터인 MNIST와 CNN의 모형을 사용하여 Bayesian optimizer를 적용해보았다. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2BPfVdr9f_M",
    "outputId": "77a9507c-f5d8-43cb-f4c5-6cd6214d49ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Bayesian-Optimization in c:\\users\\kanii\\anaconda3\\envs\\tf0\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\kanii\\anaconda3\\envs\\tf0\\lib\\site-packages (from Bayesian-Optimization) (0.22)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\kanii\\anaconda3\\envs\\tf0\\lib\\site-packages (from Bayesian-Optimization) (1.18.0)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\kanii\\anaconda3\\envs\\tf0\\lib\\site-packages (from Bayesian-Optimization) (1.3.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\kanii\\anaconda3\\envs\\tf0\\lib\\site-packages (from scikit-learn>=0.18.0->Bayesian-Optimization) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install Bayesian-Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Af0haMUB8Bph"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint , EarlyStopping\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score #얘는 그냥 cv score만 반환\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from bayes_opt import BayesianOptimization\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# MNIST Dataset 을 가져오기\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcjQJp_O_gg7"
   },
   "source": [
    "## MNIST data 불러오기\n",
    "\n",
    "keras library로 부터 데이터를 불러오고, cnn을 적용하기 위한 처리를 진행.\n",
    "1. label <- one - hot encoding\n",
    "2. Reshape data and scalling into [0,1] # Keras 모형은 input이 0~1사이일 때 가장 잘 작동.\n",
    "3. K-fold CV을 사용하기 위해 folder를 생성함\n",
    "   -> MNIST는 classification problem임으로, 어떤 폴더에 특정 라벨들이 과다 배정되는 것을 막기 위해, Stratified K- Fold 함수를 사용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQ7yrC5l-xYj",
    "outputId": "0f1592ed-4928-452b-b15a-bb0e3182c333"
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 먼저 CV folder를 생성.\n",
    "e_tr=LabelEncoder()\n",
    "Y_train_i=e_tr.fit_transform(y_train)\n",
    "\n",
    "e_te=LabelEncoder()\n",
    "Y_test_i=e_te.fit_transform(y_test)\n",
    "\n",
    "skf=StratifiedKFold(n_splits=5,random_state=2021,shuffle=True) # k=5\n",
    "\n",
    "# 1. Label One-Hot Encoding \n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# 2. Reshape data and scalling\n",
    "x_train = x_train.reshape(60000, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, 28, 28, 1).astype('float32') / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 개인 컴퓨터의 사양이 좋지 못한 관계로, Train과 Test data set의 사이즈를 각각 $\\frac{1}{10}$로 줄여서 사용하였습니다.\n",
    "\n",
    "+ 데이터는 계층적 랜덤 추출을 사용해, 각 그룹들이 같은 비율로 추출되도록 하였습니다.\n",
    "\n",
    "$\\therefore$ N.train = 6,000  N.test = 1,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 592, 1: 674, 2: 596, 3: 613, 4: 584, 5: 542, 6: 592, 7: 627, 8: 585, 9: 595}\n",
      "{0: 98, 1: 114, 2: 103, 3: 101, 4: 98, 5: 89, 6: 96, 7: 103, 8: 97, 9: 101}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split=StratifiedShuffleSplit(n_splits=1,test_size=0.1,random_state=2021)\n",
    "\n",
    "#train_index= split.split(x_train,Y_train_i)\n",
    "\n",
    "for train_index, test_index in split.split(x_train,Y_train_i) :\n",
    "    x_train=x_train[test_index];y_train=y_train[test_index];Y_train_i=Y_train_i[test_index]\n",
    "\n",
    "unique_train, counts_train = np.unique(Y_train_i, return_counts = True) \n",
    "uniq_train = dict(zip(unique_train, counts_train))\n",
    "\n",
    "for train_index, test_index in split.split(x_test,Y_test_i):\n",
    "    x_test=x_test[test_index];y_test=y_test[test_index];Y_test_i=Y_test_i[test_index]\n",
    "    \n",
    "unique_test, counts_test = np.unique(Y_test_i, return_counts = True) \n",
    "uniq_test = dict(zip(unique_test, counts_test))\n",
    "\n",
    "\n",
    "print(uniq_train)\n",
    "print(uniq_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dlCExXmDbCr"
   },
   "source": [
    "Bayesian Optimizer를 적용하기 전에 먼저 모형을 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(n_conv_layers,n_fully_connected_layers,n_cmaps,n_node,dr,ks,ps,lr):\n",
    "        #Activation function  : relu\n",
    "    act_fn='relu'\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(int(n_cmaps),kernel_size=int(ks),input_shape=(28,28,1),activation=act_fn\n",
    "                                 ,padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=int(ps),padding='same'))\n",
    "    model.add(Dropout(dr))\n",
    "            \n",
    "    for index , layer in enumerate(np.arange(int(n_conv_layers))): # Convolution Layer 개수 만큼 층을 쌓는 부분\n",
    "        model.add(Conv2D(int(n_cmaps),kernel_size=int(ks),activation=act_fn\n",
    "                                 ,padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=int(ps),padding='same'))\n",
    "        model.add(Dropout(dr))\n",
    "            \n",
    "    model.add(Flatten())\n",
    "\n",
    "    for index , layer in enumerate(np.arange(int(n_fully_connected_layers))): # Flatten layer 개수 만큼 층을 쌓는 부분.\n",
    "        model.add(Dense(int(n_node),activation=act_fn))\n",
    "        model.add(Dropout(dr))\n",
    "\n",
    "    model.add(Dense(10,activation=\"softmax\"))\n",
    "    \n",
    "    optimizer=optimizers.Adam(learning_rate=lr) # Optimizer : Adam \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(n_conv_layers,n_fully_connected_layers,n_cmaps,n_node,dr,ks,ps,lr,epoch):\n",
    "    model=my_model(n_conv_layers,n_fully_connected_layers,n_cmaps,n_node,dr,ks,ps,lr)\n",
    "    \n",
    "    scores=np.zeros(5)\n",
    "    for i, (idx_train,idx_test) in enumerate(skf.split(x_train,Y_train_i)):\n",
    "        X_train_k=x_train[idx_train]\n",
    "        Y_train_k=y_train[idx_train]\n",
    "        X_test_k=x_train[idx_test]\n",
    "        Y_test_k=Y_train_i[idx_test]\n",
    "\n",
    "        model.fit(X_train_k,Y_train_k,epochs=int(epoch),verbose=0)\n",
    "        Yhat=np.argmax(model.predict(X_test_k), axis=-1)\n",
    "\n",
    "        acc_k=np.mean(Yhat==Y_test_k) #accuracy for k-th folder\n",
    "\n",
    "        scores[i]=acc_k\n",
    "        \n",
    "    ave_score=np.mean(scores)\n",
    "    \n",
    "    return ave_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_M7Z0tGPAFP"
   },
   "source": [
    "#### Bayesian Optimaizer를 통해 추정 할 하이퍼 파라미터 리스트.\n",
    "1. n_conv_layers : Convolution Layer의 수\n",
    "2. n_fully_connected_layers : Fully connected Layer의 수\n",
    "3. n_cmaps : Convolution layer 의 노드 수 \n",
    "4. n_node : Fully connected Layer의 노드 수\n",
    "5. dr : Convolution layer part에서 dropout rate.\n",
    "6. ps : Convolution layer part에서 polling size\n",
    "7. lr : Learning rate\n",
    "8. epoch\n",
    "\n",
    "+ Bayesian Optimizer 같은 경우는 numerical parameter에 대해서만 작동하므로, \n",
    "Activation function 과 같은 character parameter는 가장 일반적인 것으로 사용하였음.\n",
    "-> Activation function : relu\n",
    "-> Optimizer : Adam\n",
    "-> Polling method : Max Pooling\n",
    "-> Loss : Categorical Cross Entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-wvy27LQxUZ"
   },
   "source": [
    "## What is 'Bayesian Optimizer' ?\n",
    "\n",
    " Bayesian Optimization (이하 '베이지안 최적화') 이란 어느 입력 값 x를 받는 미지의 목적함수 $f$ 를 상정하여 그 함수 값 $f(x)$를 최대로 하는 최적해를 찾는 방법이다. 이 방법에는 2가지의 필수 요소가 존재하는데, 첫번째는 Surrogate Model이고, 다른 하나는 Acquisition Fuction이다.\n",
    "\n",
    " > **Surrogate Model** 이란 미지의 목적 함수 $f(x)$에 대한 확률 추정 과정을 수행하는 모델이다. 쉽게 말해, 베이지안 통계 분석에서 쓰이는 Prior distribution 과 같은 역할을 한다고 볼 수 있다. 본 과제에서는 가장 일반적으로 많이 사용되는 GP(Gaussian Process)를 Surrogate Model로 사용하였다.\n",
    "\n",
    " > **Acquisition Function** 이란 Surrogate Model의 확률 분포하에서 해당 시점의 확률 분포 ($t$시점까지 계산된 확률 분포)에서 지금 까지 나온 값들보다 더 큰 값이 나올 가능성이 높은 point를 알려주는 함수 이다. 본 과제에서는 EI (Expected Imporvement) 를 Acquisition function으로 사용하였다.\n",
    ">\n",
    ">EI function에 대해 더 자세히 언급해보자면,\n",
    ">\n",
    "> 다음 후보 점을 찾기 위해서는 크게 2가지 전략(Exploration 과 Expectation)이 존재한다. Exploration은 불확실성이 가장 높은 곳에서 지금 까지 나온 값들보다 더 좋은 포인트가 있을 것이다 라는 전략이고, Expectation은 지금까지 나온 포인트들 중 높은 값들 근처에 더 높은 포인트가 있을 것이라는 전략이다.\n",
    ">\n",
    "> 이 두 가지의 전략에서 적절하게 균형 잡힌 포인트를 얻을 수 있도록 설계된 Acqusition function이 바로 Expected Improvement Acqusition function이다. 수식적인 표현은 아래 그림과 같다.\n",
    "\n",
    "\n",
    " ![AC.JPG](https://drive.google.com/uc?id=1kaU4uuAkq7PGs-_0NRuGeFax3MCageyr)\n",
    "\n",
    "\n",
    "> **베이지안 최적화의 작동 과정**\n",
    "\n",
    "![BOworking.png](https://drive.google.com/uc?id=1tnltlgArggXkRvB5BLShO8tONyYRVH25)\n",
    "\n",
    "검은색 점선 : Target function\n",
    "검은색 실선 : Surrogate Model\n",
    "파란색 음영 : Confidence Interval\n",
    "하단부 초록색 실선 : Acquisition Function\n",
    "\n",
    "검은색 점 : 관측된 점\n",
    "\n",
    "시점 $t$ 를 반복 되면서, Surrogate Model의 불확실성 (파란색 음영) 이 점점 줄어들고, Surrogate Model이 Target function에 근사해지며, 매 시점 $t$에서 다음 입력 값 후보는 Acqusition function을 최대화 하는 점으로 추천 된다.\n",
    "\n",
    "베이지안 최적화는 확률 과정이라는 점에서 Random Search와 유사하지만, 이전 후보 입력 값들의 결과를 반영하여 다음 입력 값이 추천된다는 점에서 Random Search보다 더 근거 있는 하이퍼-파라미터 조합을 얻을 수 있다는 장점이 있다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W6LSfi9P-xy6",
    "outputId": "e7160b1e-e419-4e88-9cb7-25e9824c3b4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    dr     |   epoch   |    ks     |    lr     |  n_cmaps  | n_conv... | n_full... |  n_node   |    ps     |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9883  \u001b[0m | \u001b[0m 0.2463  \u001b[0m | \u001b[0m 81.34   \u001b[0m | \u001b[0m 2.417   \u001b[0m | \u001b[0m 0.003814\u001b[0m | \u001b[0m 99.81   \u001b[0m | \u001b[0m 1.641   \u001b[0m | \u001b[0m 1.895   \u001b[0m | \u001b[0m 83.2    \u001b[0m | \u001b[0m 3.986   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.1123  \u001b[0m | \u001b[0m 0.3159  \u001b[0m | \u001b[0m 36.78   \u001b[0m | \u001b[0m 2.176   \u001b[0m | \u001b[0m 0.009662\u001b[0m | \u001b[0m 73.93   \u001b[0m | \u001b[0m 1.433   \u001b[0m | \u001b[0m 3.806   \u001b[0m | \u001b[0m 73.92   \u001b[0m | \u001b[0m 4.892   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9605  \u001b[0m | \u001b[0m 0.03836 \u001b[0m | \u001b[0m 79.71   \u001b[0m | \u001b[0m 3.55    \u001b[0m | \u001b[0m 0.007623\u001b[0m | \u001b[0m 98.86   \u001b[0m | \u001b[0m 2.563   \u001b[0m | \u001b[0m 1.23    \u001b[0m | \u001b[0m 82.99   \u001b[0m | \u001b[0m 3.268   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.991   \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 6.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 2.0     \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9845  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6615  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 6.0     \u001b[0m | \u001b[0m 68.9    \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9892  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 77.29   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.957   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9837  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 63.55   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9708  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9857  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 57.21   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 81.95   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 66.45   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m 0.9918  \u001b[0m | \u001b[95m 0.4     \u001b[0m | \u001b[95m 81.72   \u001b[0m | \u001b[95m 2.0     \u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 32.0    \u001b[0m | \u001b[95m 2.0     \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6148  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 76.06   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 6.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9823  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m |\n",
      "| \u001b[95m 16      \u001b[0m | \u001b[95m 0.9933  \u001b[0m | \u001b[95m 0.4     \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 2.0     \u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 66.35   \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 32.0    \u001b[0m | \u001b[95m 2.0     \u001b[0m |\n",
      "| \u001b[95m 17      \u001b[0m | \u001b[95m 0.9943  \u001b[0m | \u001b[95m 0.4     \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 2.0     \u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 69.16   \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 2.0     \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.9913  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 47.33   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 55.13   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m |\n",
      "| \u001b[95m 19      \u001b[0m | \u001b[95m 0.9952  \u001b[0m | \u001b[95m 0.4     \u001b[0m | \u001b[95m 91.78   \u001b[0m | \u001b[95m 2.0     \u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 2.0     \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.5155  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 44.08   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 6.0     \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.9892  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.993   \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 66.84   \u001b[0m | \u001b[0m 2.0     \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "Iteration 0: \n",
      "\t{'target': 0.9883333333333333, 'params': {'dr': 0.24633152873488787, 'epoch': 81.33585528354487, 'ks': 2.4168414701851963, 'lr': 0.003814057754692183, 'n_cmaps': 99.81254313114167, 'n_conv_layers': 1.6408118770948035, 'n_fully_connected_layers': 1.89496552975094, 'n_node': 83.19892915677437, 'ps': 3.986481542928602}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.11233333333333333, 'params': {'dr': 0.31588095153503787, 'epoch': 36.78260771474026, 'ks': 2.175713855264108, 'lr': 0.009661563911893527, 'n_cmaps': 73.92590618483104, 'n_conv_layers': 1.433149806242736, 'n_fully_connected_layers': 3.8063618125009833, 'n_node': 73.92368018602333, 'ps': 4.891529068626896}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.9605, 'params': {'dr': 0.0383638274140288, 'epoch': 79.70636593595476, 'ks': 3.549759037873204, 'lr': 0.007623200163958603, 'n_cmaps': 98.85638663330977, 'n_conv_layers': 2.5627776248258156, 'n_fully_connected_layers': 1.2299360272615063, 'n_node': 82.99424811631174, 'ps': 3.2678020821338807}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.991, 'params': {'dr': 0.01, 'epoch': 100.0, 'ks': 5.0, 'lr': 0.0010000266675536083, 'n_cmaps': 100.0, 'n_conv_layers': 6.0, 'n_fully_connected_layers': 1.0, 'n_node': 100.0, 'ps': 2.0}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.9845, 'params': {'dr': 0.4, 'epoch': 100.0, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 100.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 32.0, 'ps': 5.0}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.6615, 'params': {'dr': 0.01, 'epoch': 100.0, 'ks': 2.0, 'lr': 0.01, 'n_cmaps': 100.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 6.0, 'n_node': 68.90452653522577, 'ps': 5.0}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.9891666666666665, 'params': {'dr': 0.4, 'epoch': 77.29123269431335, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 100.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 100.0, 'ps': 5.0}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.9570000000000001, 'params': {'dr': 0.4, 'epoch': 100.0, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 32.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 32.0, 'ps': 5.0}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.9836666666666666, 'params': {'dr': 0.4, 'epoch': 63.55337031650072, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 100.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 32.0, 'ps': 5.0}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.9708333333333334, 'params': {'dr': 0.4, 'epoch': 100.0, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 32.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 100.0, 'ps': 5.0}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.9856666666666667, 'params': {'dr': 0.4, 'epoch': 57.21262053704134, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 32.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 32.0, 'ps': 2.0}}\n",
      "Iteration 11: \n",
      "\t{'target': 0.99, 'params': {'dr': 0.4, 'epoch': 81.94892322789647, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 66.45115207225057, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 32.0, 'ps': 2.0}}\n",
      "Iteration 12: \n",
      "\t{'target': 0.9918333333333333, 'params': {'dr': 0.4, 'epoch': 81.71661651030355, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 100.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 32.0, 'ps': 2.0}}\n",
      "Iteration 13: \n",
      "\t{'target': 0.6148333333333333, 'params': {'dr': 0.4, 'epoch': 76.06003065651635, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 32.0, 'n_conv_layers': 6.0, 'n_fully_connected_layers': 1.0, 'n_node': 32.0, 'ps': 5.0}}\n",
      "Iteration 14: \n",
      "\t{'target': 0.9823333333333334, 'params': {'dr': 0.4, 'epoch': 30.0, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 32.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 32.0, 'ps': 2.0}}\n",
      "Iteration 15: \n",
      "\t{'target': 0.9933333333333334, 'params': {'dr': 0.4, 'epoch': 100.0, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 66.34891971018789, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 32.0, 'ps': 2.0}}\n",
      "Iteration 16: \n",
      "\t{'target': 0.9943333333333333, 'params': {'dr': 0.4, 'epoch': 100.0, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 69.16393438195367, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 100.0, 'ps': 2.0}}\n",
      "Iteration 17: \n",
      "\t{'target': 0.9913333333333334, 'params': {'dr': 0.4, 'epoch': 47.32983273374017, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 55.13228767360181, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 32.0, 'ps': 2.0}}\n",
      "Iteration 18: \n",
      "\t{'target': 0.9951666666666666, 'params': {'dr': 0.4, 'epoch': 91.78453354631536, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 100.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 100.0, 'ps': 2.0}}\n",
      "Iteration 19: \n",
      "\t{'target': 0.5155000000000001, 'params': {'dr': 0.4, 'epoch': 44.084683136555924, 'ks': 5.0, 'lr': 0.001, 'n_cmaps': 32.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 6.0, 'n_node': 32.0, 'ps': 2.0}}\n",
      "Iteration 20: \n",
      "\t{'target': 0.9891666666666665, 'params': {'dr': 0.4, 'epoch': 30.0, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 100.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 32.0, 'ps': 2.0}}\n",
      "Iteration 21: \n",
      "\t{'target': 0.993, 'params': {'dr': 0.4, 'epoch': 100.0, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 32.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 66.84389268851363, 'ps': 2.0}}\n",
      "{'target': 0.9951666666666666, 'params': {'dr': 0.4, 'epoch': 91.78453354631536, 'ks': 2.0, 'lr': 0.001, 'n_cmaps': 100.0, 'n_conv_layers': 1.0, 'n_fully_connected_layers': 1.0, 'n_node': 100.0, 'ps': 2.0}}\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameter 별 범위 설정\n",
    "## 파라미터 별 범위는 실험자가 manual하게 설정.\n",
    "pbound={\n",
    "    'n_conv_layers':(1,6),'n_fully_connected_layers':(1,6),\n",
    "    'n_cmaps':(32,100),'n_node': (32,100),'lr': (0.001,0.01),'dr': (0.01,0.4),\n",
    "    'ks':(2,5),'ps':(2,5),'epoch':(30,100)}\n",
    "\n",
    "optimizer=BayesianOptimization(\n",
    "    f=my_function, # Target 함수\n",
    "    pbounds=pbound, # Hyper parameter의 집합\n",
    "    verbose=2,random_state=2021)\n",
    "\n",
    "\n",
    "optimizer.maximize(init_points=2,n_iter=20,xi=0.01,acq='ei') \n",
    "# init_points : random 하게 몇개의 초기값을 잡을 것인지\n",
    "# n_iter : iteration 횟수, 많을 수록 더 정확한 값을 얻을 수 있다.\n",
    "# xi : 어느 정도의 강도로 expoloration을 고려할 것인지. (Defalut = 0.00)\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "fYI8kjL4SLQQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.9951666666666666,\n",
       " 'params': {'dr': 0.4,\n",
       "  'epoch': 91.78453354631536,\n",
       "  'ks': 2.0,\n",
       "  'lr': 0.001,\n",
       "  'n_cmaps': 100.0,\n",
       "  'n_conv_layers': 1.0,\n",
       "  'n_fully_connected_layers': 1.0,\n",
       "  'n_node': 100.0,\n",
       "  'ps': 2.0}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최적화 결과\n",
    "optimizer.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 베이지안 최적화 결과: CV-Accuracy = 0.9952\n",
    "+ drop rate : 0.4\n",
    "+ epoch : 91\n",
    "+ kernel size : 2\n",
    "+ Polling size : 2\n",
    "+ Learning rate : 0.001\n",
    "+ Convolution layer의 node 수 : 100\n",
    "+ Convolution Layer의 수 : 1\n",
    "+ Flatten layer의 node 수 : 100\n",
    "+ Flatten layer의 수 : 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "Du5JE_KKSLS6"
   },
   "outputs": [],
   "source": [
    "# Dictionary 저장\n",
    "np.save('Optimum.npy',optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECctc7TDSLVs"
   },
   "outputs": [],
   "source": [
    "# load 하는 법\n",
    "# optimum = np.load('Optimum.npy',allow_pickle='TRUE')\n",
    "# print(optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "7eih1D5N-x1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_85 (Conv2D)           (None, 28, 28, 100)       500       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_83 (MaxPooling (None, 14, 14, 100)       0         \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 14, 14, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_86 (Conv2D)           (None, 14, 14, 100)       40100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_84 (MaxPooling (None, 7, 7, 100)         0         \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 7, 7, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 100)               490100    \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 531,710\n",
      "Trainable params: 531,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 베이지안 최적화로 튜닝된 최종 모형 \n",
    "Final_model=my_model(1,1,100,100,0.4,2,2,0.001)\n",
    "Final_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples\n",
      "Epoch 1/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.8740 - acc: 0.7105\n",
      "Epoch 2/91\n",
      "6000/6000 [==============================] - 15s 2ms/sample - loss: 0.3487 - acc: 0.8887\n",
      "Epoch 3/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.2458 - acc: 0.9218\n",
      "Epoch 4/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.1980 - acc: 0.9415\n",
      "Epoch 5/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.1692 - acc: 0.9470\n",
      "Epoch 6/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.1453 - acc: 0.9553\n",
      "Epoch 7/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.1288 - acc: 0.9612\n",
      "Epoch 8/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.1038 - acc: 0.9648\n",
      "Epoch 9/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.1060 - acc: 0.9643\n",
      "Epoch 10/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0957 - acc: 0.9697\n",
      "Epoch 11/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0865 - acc: 0.9692\n",
      "Epoch 12/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0692 - acc: 0.9758\n",
      "Epoch 13/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0725 - acc: 0.9753\n",
      "Epoch 14/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0713 - acc: 0.9763\n",
      "Epoch 15/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0644 - acc: 0.9782\n",
      "Epoch 16/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0647 - acc: 0.9762\n",
      "Epoch 17/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0532 - acc: 0.9817\n",
      "Epoch 18/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0525 - acc: 0.9840\n",
      "Epoch 19/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0540 - acc: 0.9803\n",
      "Epoch 20/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0499 - acc: 0.9827\n",
      "Epoch 21/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0450 - acc: 0.9855\n",
      "Epoch 22/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0455 - acc: 0.9840\n",
      "Epoch 23/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0459 - acc: 0.9845\n",
      "Epoch 24/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0439 - acc: 0.9833\n",
      "Epoch 25/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0438 - acc: 0.9853\n",
      "Epoch 26/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0402 - acc: 0.9862\n",
      "Epoch 27/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0338 - acc: 0.9890\n",
      "Epoch 28/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0387 - acc: 0.9847\n",
      "Epoch 29/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0331 - acc: 0.9880\n",
      "Epoch 30/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0361 - acc: 0.9860\n",
      "Epoch 31/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0366 - acc: 0.9872\n",
      "Epoch 32/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0318 - acc: 0.9882\n",
      "Epoch 33/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0407 - acc: 0.9857\n",
      "Epoch 34/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0365 - acc: 0.9878\n",
      "Epoch 35/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0292 - acc: 0.9892\n",
      "Epoch 36/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0263 - acc: 0.9902\n",
      "Epoch 37/91\n",
      "6000/6000 [==============================] - 15s 3ms/sample - loss: 0.0334 - acc: 0.9885\n",
      "Epoch 38/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0237 - acc: 0.9925\n",
      "Epoch 39/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0274 - acc: 0.9907\n",
      "Epoch 40/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0305 - acc: 0.9895\n",
      "Epoch 41/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0310 - acc: 0.9895\n",
      "Epoch 42/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0327 - acc: 0.9895\n",
      "Epoch 43/91\n",
      "6000/6000 [==============================] - 16s 3ms/sample - loss: 0.0327 - acc: 0.9902\n",
      "Epoch 44/91\n",
      "6000/6000 [==============================] - 15s 2ms/sample - loss: 0.0267 - acc: 0.9910\n",
      "Epoch 45/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0228 - acc: 0.9920\n",
      "Epoch 46/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0331 - acc: 0.9892\n",
      "Epoch 47/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0270 - acc: 0.9912\n",
      "Epoch 48/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0286 - acc: 0.9918\n",
      "Epoch 49/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0303 - acc: 0.9883\n",
      "Epoch 50/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0273 - acc: 0.9913\n",
      "Epoch 51/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0325 - acc: 0.9880\n",
      "Epoch 52/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0195 - acc: 0.9943\n",
      "Epoch 53/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0170 - acc: 0.9940\n",
      "Epoch 54/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0217 - acc: 0.9930\n",
      "Epoch 55/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0208 - acc: 0.9917\n",
      "Epoch 56/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0246 - acc: 0.9922\n",
      "Epoch 57/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0232 - acc: 0.9923\n",
      "Epoch 58/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0213 - acc: 0.9935\n",
      "Epoch 59/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0237 - acc: 0.9915\n",
      "Epoch 60/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0208 - acc: 0.9932\n",
      "Epoch 61/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0218 - acc: 0.9928\n",
      "Epoch 62/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0289 - acc: 0.9910\n",
      "Epoch 63/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0196 - acc: 0.9935\n",
      "Epoch 64/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0197 - acc: 0.9935\n",
      "Epoch 65/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0225 - acc: 0.9935\n",
      "Epoch 66/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0212 - acc: 0.9935\n",
      "Epoch 67/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0203 - acc: 0.9930\n",
      "Epoch 68/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0275 - acc: 0.9927\n",
      "Epoch 69/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0195 - acc: 0.9927\n",
      "Epoch 70/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0258 - acc: 0.9918\n",
      "Epoch 71/91\n",
      "6000/6000 [==============================] - 15s 2ms/sample - loss: 0.0182 - acc: 0.9933\n",
      "Epoch 72/91\n",
      "6000/6000 [==============================] - 15s 2ms/sample - loss: 0.0197 - acc: 0.9922\n",
      "Epoch 73/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0145 - acc: 0.9958\n",
      "Epoch 74/91\n",
      "6000/6000 [==============================] - 15s 2ms/sample - loss: 0.0182 - acc: 0.9938\n",
      "Epoch 75/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0255 - acc: 0.9908\n",
      "Epoch 76/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0147 - acc: 0.9942\n",
      "Epoch 77/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0184 - acc: 0.9940\n",
      "Epoch 78/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0172 - acc: 0.9947\n",
      "Epoch 79/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0160 - acc: 0.9943\n",
      "Epoch 80/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0193 - acc: 0.9937\n",
      "Epoch 81/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0194 - acc: 0.9933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/91\n",
      "6000/6000 [==============================] - 13s 2ms/sample - loss: 0.0195 - acc: 0.9930\n",
      "Epoch 83/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0152 - acc: 0.9942\n",
      "Epoch 84/91\n",
      "6000/6000 [==============================] - 13s 2ms/sample - loss: 0.0250 - acc: 0.9915\n",
      "Epoch 85/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0138 - acc: 0.9953\n",
      "Epoch 86/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0180 - acc: 0.9935\n",
      "Epoch 87/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0217 - acc: 0.9925\n",
      "Epoch 88/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0160 - acc: 0.9940\n",
      "Epoch 89/91\n",
      "6000/6000 [==============================] - 13s 2ms/sample - loss: 0.0156 - acc: 0.9957\n",
      "Epoch 90/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0276 - acc: 0.9915\n",
      "Epoch 91/91\n",
      "6000/6000 [==============================] - 14s 2ms/sample - loss: 0.0158 - acc: 0.9942\n"
     ]
    }
   ],
   "source": [
    "# 전체 train data set을 사용하여 모형을 학습.\n",
    "history = Final_model.fit(x_train,y_train,epochs=91,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loss=history.history['loss']\n",
    "y_acc=history.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_len=np.arange(len(y_loss))\n",
    "plt.plot(x_len,y_loss,marker=\".\",c=\"red\",label=\"Train_Loss\")\n",
    "plt.plot(x_len,y_acc,marker=\".\",c=\"green\",label=\"Train_Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction for test data set\n",
    "y_pred=np.argmax(Final_model.predict(x_test), axis=-1)\n",
    "# Compute test accuracy\n",
    "test_acc=np.mean(y_pred==Y_test_i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.981\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 98,   0,   1,   0,   0,   0,   1,   0,   1,   0],\n",
       "       [  0, 114,   1,   0,   0,   0,   0,   0,   0,   1],\n",
       "       [  0,   0, 101,   1,   0,   0,   0,   1,   0,   1],\n",
       "       [  0,   0,   0,  99,   0,   1,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,  94,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,  87,   1,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   1,   0,  94,   0,   0,   1],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 102,   1,   0],\n",
       "       [  0,   0,   0,   1,   0,   0,   0,   0,  95,   1],\n",
       "       [  0,   0,   0,   0,   3,   1,   0,   0,   0,  97]], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_pred,Y_test_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 Test Data set에 대한 Test Accuarcy = 0.981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    " 베이지안 최적화를 통해 적합된 CNN 모형은 Test data set에 대한 Accuracy 가 0.981 아주 좋은 성능을 보였다.\n",
    "비록 간단한 MNIST 데이터에 대한 결과이지만, 이러한 결과는 베이지안 최적화가 널리 사용되고 있는 Grid search나 randomized search만큼 충분히 모형 튜닝에서 좋은 성능을 보인다는 것을 알 수 있게한다.\n",
    "\n",
    "다만, 베이지안 최적화는 아직 numerical parameter에 대해서만 작동한다는 단점이 있다.\n",
    "후에 이러한 단점이 더 보완된다면, 베이지안 최적화는 딥러닝 방법론들을 투명화 하는 하나의 방법으로 널리 사용될 수 있을 것이다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "딥러닝 개인과제",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
